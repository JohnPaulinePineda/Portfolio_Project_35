---
title: "Supervised Learning : Characterizing Life Expectancy Drivers Across Countries Using Model-Agnostic Interpretation Methods for Black-Box Models"
author: "John Pauline Pineda"
date: "July 20, 2023"
output: 
  html_document:
    toc: true
    toc_depth: 4
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| Description
|
##  1.1 Introduction
|
###  1.1.1 Study Objectives
|
###  1.1.2 Outcome
|
###  1.1.3 Predictors
|
##  1.2 Methodology
|
###  1.2.1 Model Formulation
|
| [Stochastic Gradient Boosting](https://www.sciencedirect.com/science/article/abs/pii/S0167947301000652) is an ensemble learning method which combines multiple weak learners in an additive manner to improve prediction. The process is initialized using a decision tree base learner with the aim of minimizing a specified loss function. The negative gradient  of the loss function with respect to the predicted values from the current ensemble is calculated. Residuals are determined as the difference between the actual target values and the learner predictions. A new base learner is subsequently formulated but is trained to predict the residuals. The algorithm involves iteratively improving the ensemble by focusing on the residuals of the previous predictions. Each subsequent base learner is trained to reduce the errors made by the previous ensemble, gradually refining the model's predictive capabilities.
|
| [Random Forest](https://link.springer.com/article/10.1023/A:1010933404324) is an ensemble learning method made up of a large set of small decision trees called estimators, with each producing its own prediction. The random forest model aggregates the predictions of the estimators to produce a more accurate prediction. The algorithm involves bootstrap aggregating (where smaller subsets of the training data are repeatedly subsampled with replacement), random subspacing (where a subset of features are sampled and used to train each individual estimator), estimator training (where unpruned decision trees are formulated for each estimator) and inference (where a final prediction is formilated by aggregating the individual predictions of all estimators).
|
| [Partial Least Squares](https://epubs.siam.org/doi/10.1137/0905052) applies dimensionality reduction to address high multicollinearity among predictors in a linear regression. The algorithm calculates summary indices termed as partial least squares components which are linear combinations of the original predictors by considering the variation in both the response and the predictor variables. The method of least squares is then applied to fit a linear regression model using the first principal components as predictors, with the optimal number determined using cross-validation.
|
| [Cubist Regression](https://www.semanticscholar.org/paper/Learning-With-Continuous-Classes-Quinlan/ead572634c6f7253bf187a3e9a7dc87ae2e34258) is a rule-based model that is an extension of Quinlanâ€™s M5 model tree. A tree is grown where the terminal leaves contain linear regression models. These models are based on the predictors used in previous splits. Also, there are intermediate linear models at each step of the tree. A prediction is made using the linear regression model at the terminal node of the tree, but is smoothed by taking into account the prediction from the linear model in the previous node of the tree (which also occurs recursively up the tree). The tree is reduced to a set of rules, which initially are paths from the top of the tree to the bottom. Rules are eliminated via pruning and/or combined for simplification. The Cubist model can also use a boosting-like scheme called committees where iterative model trees are created in sequence. Another innovation is about using nearest-neighbors to adjust the predictions from the rule-based model.
|
###  1.2.2 Model Evaluation
|
###  1.2.3 Model Post-Hoc Analysis
|
##  1.3 Results
|
###  1.3.1 Data Preparation
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(DALEX)
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(skimr)
library(corrplot)
library(lares)
library(dplyr)
library(minerva)
library(CORElearn)
library(patchwork)
library(lime)
library(DALEXtra)

##################################
# Loading source and
# formulating the analysis set
##################################
LED <- read.csv("Life_Expectancy_Data.csv",
                na.strings=c("NA","NaN"," ",""),
                stringsAsFactors = FALSE)
LED <- as.data.frame(LED)

##################################
# Performing a general exploration of the data set
##################################
dim(LED)
str(LED)
summary(LED)

##################################
# Transforming to appropriate data types
##################################
LED$YEAR <- factor(LED$YEAR,
                      levels = c("2019"))
LED$GENDER <- factor(LED$GENDER,
                      levels = c("Male","Female"))
LED$CONTIN <- as.factor(LED$CONTIN)

##################################
# Reducing the range of values
# for certain numeric predictors
##################################
LED$GDP     <- LED$GDP/1000000000
LED$GNI     <- LED$GNI/1000000000
LED$PERCAP  <- LED$PERCAP/1000

##################################
# Formulating a data type assessment summary
##################################
PDA <- LED
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

</details>

###  1.3.2 Data Quality Assessment
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- LED

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all Predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("COUNTRY","YEAR","LIFEXP")]

##################################
# Listing all numeric Predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,sapply(DQA.Predictors, is.numeric), drop = FALSE]

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There is (are) ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric descriptor variable(s)."))
} else {
  print("There are no numeric descriptor variables.")
}

##################################
# Listing all factor Predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,sapply(DQA.Predictors, is.factor), drop = FALSE]

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There is (are) ",
               (length(names(DQA.Predictors.Factor))),
               " factor descriptor variable(s)."))
} else {
  print("There are no factor descriptor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor),
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric),
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor Predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor Predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric Predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric Predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric Predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric Predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric Predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric Predictors noted.")
}

```

</details>

###  1.3.3 Data Preprocessing
|
|
####  1.3.3.1 Outlier Treatment
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- LED

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))
```

```{r section_1.3.3.2, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Outlier Treatment
##################################

##################################
# Listing all Predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("COUNTRY","YEAR","LIFEXP")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  print(
  ggplot(DPA.Predictors.Numeric, aes(x=DPA.Predictors.Numeric[,i])) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) +
  xlab(names(DPA.Predictors.Numeric)[i]) +
  labs(title=names(DPA.Predictors.Numeric)[i],
       subtitle=paste0(OutlierCount, " Outlier(s) Detected")))
}

##################################
# Formulating the histogram
# for the numeric predictors
##################################

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Median <- format(round(median(DPA.Predictors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA.Predictors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA.Predictors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA.Predictors.Numeric, aes(x=DPA.Predictors.Numeric[,i])) +
  geom_histogram(binwidth=1,color="black", fill="white") +
  geom_vline(aes(xintercept=mean(DPA.Predictors.Numeric[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA.Predictors.Numeric[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA.Predictors.Numeric)[i]) +
  labs(title=names(DPA.Predictors.Numeric)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

##################################
# Investigating distributional anomalies
# observed for several predictors 
##################################
(INFMOR_Unique <- DPA %>%
  group_by(INFMOR) %>%
  summarize(Distinct_INFMOR = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_INFMOR)) %>%
  slice(1:5))
(INFMOR_Unique_Country <- DPA[round(DPA$INFMOR,digits=1)==30.2,c("COUNTRY")])

DPA %>%
  group_by(CLTECH) %>%
  summarize(Distinct_CLTECH = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_CLTECH)) %>%
  slice(1:5)
(CLTECH_Unique_Country <- DPA[round(DPA$CLTECH,digits=1)==60.6,c("COUNTRY")])

DPA %>%
  group_by(RTIMOR) %>%
  summarize(Distinct_RTIMOR = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_RTIMOR)) %>%
  slice(1:5)
(RTIMOR_Unique_Country <- DPA[round(DPA$RTIMOR,digits=1)==18.2,c("COUNTRY")])

DPA %>%
  group_by(DPTIMM) %>%
  summarize(Distinct_DPTIMM = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_DPTIMM)) %>%
  slice(1:5)
(DPTIMM_Unique_Country <- DPA[round(DPA$DPTIMM,digits=1)==85.7,c("COUNTRY")])

DPA %>%
  group_by(HEPIMM) %>%
  summarize(Distinct_HEPIMM = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_HEPIMM)) %>%
  slice(1:5)
(HEPIMM_Unique_Country <- DPA[round(DPA$HEPIMM,digits=1)==81.3,c("COUNTRY")])

DPA %>%
  group_by(MEAIMM) %>%
  summarize(Distinct_MEAIMM = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_MEAIMM)) %>%
  slice(1:5)
(MEAIMM_Unique_Country <- DPA[round(DPA$MEAIMM,digits=1)==84.9,c("COUNTRY")])

DPA %>%
  group_by(HOSBED) %>%
  summarize(Distinct_HOSBED = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_HOSBED)) %>%
  slice(1:5)
(HOSBED_Unique_Country <- DPA[round(DPA$HOSBED,digits=1)==3.0,c("COUNTRY")])

DPA %>%
  group_by(NCOMOR) %>%
  summarize(Distinct_NCOMOR = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_NCOMOR)) %>%
  slice(1:5)
(NCOMOR_Unique_Country <- DPA[round(DPA$NCOMOR,digits=1)==22.1,c("COUNTRY")])

DPA %>%
  group_by(SUIRAT) %>%
  summarize(Distinct_SUIRAT = n_distinct(COUNTRY)) %>%
  arrange(desc(Distinct_SUIRAT)) %>%
  slice(1:5)
(SUIRAT_Unique_Country <- DPA[round(DPA$SUIRAT,digits=1)==10.6,c("COUNTRY")])

(AnomalousVariables_Unique_Country <- MEAIMM_Unique_Country)

##################################
# Removing associated rows associated
# with anomalous variables
##################################
dim(DPA)

DPA <- DPA[!(DPA$COUNTRY %in% AnomalousVariables_Unique_Country),]
dim(DPA)

##################################
# Listing all Predictors
# for the updated data
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("COUNTRY","YEAR","LIFEXP")]

##################################
# Listing all numeric predictors
# for the updated data
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

```

</details>

|
|
####  1.3.3.2 Zero and Near-Zero Variance
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.3, warning=FALSE, message=FALSE}
##################################
# Zero and Near-Zero Variance
##################################

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 80/20,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance predictors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

}

```

</details>

|
|
####  1.3.3.3 Collinearity
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.4, warning=FALSE, message=FALSE}
##################################
# Collinearity
##################################

##################################
# Visualizing pairwise correlation between predictors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = .95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"), 
         method = "circle",
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.cex = 0.75,
         tl.srt = 90, 
         sig.level = 0.05, 
         p.mat = DPA_CorrelationTest$p,
         insig = "blank")

##################################
# Identifying the highly correlated variables
##################################
DPA_Correlation <-  cor(DPA.Predictors.Numeric, 
                        method = "pearson",
                        use="pairwise.complete.obs")
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)]) > 0.75))

if (DPA_HighlyCorrelatedCount == 0) {
  print("No highly correlated predictors noted.")
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.75."))
  
  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05, 
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))
  
}

if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.75)
  
  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))
  
  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }
  
}

```

</details>

|
|
####  1.3.3.4 Linear Dependencies
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.5, warning=FALSE, message=FALSE}
##################################
# Linear Dependencies
##################################

##################################
# Finding linear dependencies
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

}

```

</details>

|
|
####  1.3.3.5 Shape Transformation
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.6, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Shape Transformation
##################################

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

for (i in 1:ncol(DPA_BoxCoxTransformed)) {
  Median <- format(round(median(DPA_BoxCoxTransformed[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA_BoxCoxTransformed[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA_BoxCoxTransformed[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA_BoxCoxTransformed, aes(x=DPA_BoxCoxTransformed[,i])) +
  geom_histogram(binwidth=1,color="black", fill="white") +
  geom_vline(aes(xintercept=mean(DPA_BoxCoxTransformed[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA_BoxCoxTransformed[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA_BoxCoxTransformed)[i]) +
  labs(title=names(DPA_BoxCoxTransformed)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

DPA_BoxCoxTransformed <- cbind(DPA_BoxCoxTransformed,DPA[,c("COUNTRY",
                                                            "YEAR",
                                                            "GENDER",
                                                            "CONTIN",
                                                            "LIFEXP")])

```

</details>

|
|
####  1.3.3.6 Pre-Processed Dataset
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.7, warning=FALSE, message=FALSE}
##################################
# Creating the pre-modelling
# train set
##################################
PMA <- DPA_BoxCoxTransformed[,!names(DPA_BoxCoxTransformed) %in% c("YEAR",
                                                                   "GNI",
                                                                   "DPTIMM",
                                                                   "MEAIMM",
                                                                   "RURPOP",
                                                                   "SANSER",
                                                                   "RTIMOR")]

##################################
# Gathering descriptive statistics
##################################
(PMA_Skimmed <- skim(PMA))

```

</details>

###  1.3.4 Data Exploration
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
PME <- PMA
PME.Numeric <- PME[,sapply(PME, is.numeric), drop = FALSE]

##################################
# Listing all Predictors
##################################
PME.Predictors <- PME[,!names(PME) %in% c("COUNTRY","LIFEXP")]

##################################
# Listing all numeric Predictors
##################################
PME.Predictors.Numeric <- PME.Predictors[,sapply(PME.Predictors, is.numeric), drop = FALSE]
ncol(PME.Predictors.Numeric)

##################################
# Listing all numeric Predictors
##################################
PME.Predictors.Factor <- PME.Predictors[,sapply(PME.Predictors, is.factor), drop = FALSE]
ncol(PME.Predictors.Factor)

##################################
# Formulating the scatter plot
##################################
featurePlot(x = PME.Predictors.Numeric, 
            y = PME$LIFEXP,
            plot = "scatter",
            type = c("p", "smooth"),
            span = .5,
            layout = c(4, 3))

##################################
# Formulating the box plot
##################################
featurePlot(x = PME.Numeric, 
            y = PME$GENDER,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5,
            layout = c(4, 4))

##################################
# Formulating the box plot
##################################
featurePlot(x = PME.Numeric, 
            y = PME$CONTIN,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5,
            layout = c(4, 4))

```

</details>

###  1.3.5 Feature Selection
|
|
####  1.3.5.1 Locally Weighted Scatterplot Smoothing Pseudo-R-Squared (LOWESSPR)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.1, warning=FALSE, message=FALSE}
##################################
# Evaluating model-independent
# feature importance metrics
##################################

##################################
# Obtaining the LOWESSPR pseudo-R-Squared
##################################
FE_LOWESSPR <- filterVarImp(x = PME.Numeric[,!names(PME.Numeric) %in% c("LIFEXP")],
                            y = PME$LIFEXP,
                            nonpara = TRUE)

##################################
# Formulating the summary table
##################################
FE_LOWESSPR_Summary <- FE_LOWESSPR 

FE_LOWESSPR_Summary$Predictor <- rownames(FE_LOWESSPR)
names(FE_LOWESSPR_Summary)[1] <- "LOWESSPR"
FE_LOWESSPR_Summary$Metric <- rep("LOWESSPR",nrow(FE_LOWESSPR))

FE_LOWESSPR_Summary

##################################
# Exploring predictor performance
# using LOWESS
##################################
dotplot(Predictor ~ LOWESSPR | Metric, 
        FE_LOWESSPR_Summary,
        origin = 0,
        type = c("p", "h"),
        pch = 16,
        cex = 2,
        alpha = 0.45,
        prepanel = function(x, y) {
            list(ylim = levels(reorder(y, x)))
        },
        panel = function(x, y, ...) {
            panel.dotplot(x, reorder(y, x), ...)
        })
```

</details>

|
|
####  1.3.5.2 Pearsonâ€™s Correlation Coefficient (PCC)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.2, warning=FALSE, message=FALSE}
##################################
# Obtaining the Pearson correlation coefficient
##################################
(FE_PCC <- abs(cor(PME.Numeric, method="pearson")[-13,13]))

##################################
# Formulating the summary table
##################################
FE_PCC_Summary <- data.frame(Predictor = names(PME.Numeric)[1:(ncol(PME.Numeric)-1)],
                             PCC = FE_PCC,
                             Metric = rep("PCC", length(FE_PCC)))

FE_PCC_Summary

##################################
# Exploring predictor performance
# using PCC
##################################
dotplot(Predictor ~ PCC | Metric, 
        FE_PCC_Summary,
        origin = 0,
        type = c("p", "h"),
        pch = 16,
        cex = 2,
        alpha = 0.45,
        prepanel = function(x, y) {
            list(ylim = levels(reorder(y, x)))
        },
        panel = function(x, y, ...) {
            panel.dotplot(x, reorder(y, x), ...)
        })
```

</details>

|
|
####  1.3.5.3 Spearmanâ€™s Rank Correlation Coefficient (SRCC)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.3, warning=FALSE, message=FALSE}
##################################
# Obtaining the Spearman's rank correlation coefficient
##################################
(FE_SRCC <- abs(cor(PME.Numeric, method="spearman")[-13,13]))

##################################
# Formulating the summary table
##################################
FE_SRCC_Summary <- data.frame(Predictor = names(PME.Numeric)[1:(ncol(PME.Numeric)-1)],
                             SRCC = FE_SRCC,
                             Metric = rep("SRCC", length(FE_SRCC)))

FE_SRCC_Summary

##################################
# Exploring predictor performance
# using SRCC
##################################
dotplot(Predictor ~ SRCC | Metric, 
        FE_SRCC_Summary,
        origin = 0,
        type = c("p", "h"),
        pch = 16,
        cex = 2,
        alpha = 0.45,
        prepanel = function(x, y) {
            list(ylim = levels(reorder(y, x)))
        },
        panel = function(x, y, ...) {
            panel.dotplot(x, reorder(y, x), ...)
        })
```

</details>

|
|
####  1.3.5.4 Maximal Information Coefficient (MIC)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.4, warning=FALSE, message=FALSE}
##################################
# Obtaining the maximal information coefficient
##################################
FE_MIC <- mine(x = PME.Numeric[,!names(PME.Numeric) %in% c("LIFEXP")],
               y = PME$LIFEXP)$MIC

##################################
# Formulating the summary table
##################################
FE_MIC_Summary <- data.frame(Predictor = names(PME.Numeric)[1:(ncol(PME.Numeric)-1)],
                             MIC = FE_MIC[,1],
                             Metric = rep("MIC", length(FE_MIC)))

FE_MIC_Summary

##################################
# Exploring predictor performance
# using MIC
##################################
dotplot(Predictor ~ MIC | Metric, 
        FE_MIC_Summary,
        origin = 0,
        type = c("p", "h"),
        pch = 16,
        cex = 2,
        alpha = 0.45,
        prepanel = function(x, y) {
            list(ylim = levels(reorder(y, x)))
        },
        panel = function(x, y, ...) {
            panel.dotplot(x, reorder(y, x), ...)
        })
```

</details>

|
|
####  1.3.5.5 Relief Values (RV)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.5, warning=FALSE, message=FALSE}
##################################
# Obtaining the relief values
##################################
FE_RV <- attrEval(LIFEXP ~ .,  
                  data = PME.Numeric,
                  estimator = "RReliefFequalK")

##################################
# Formulating the summary table
##################################
FE_RV_Summary <- data.frame(Predictor = names(FE_RV),
                            RV = FE_RV,
                            Metric = rep("RV", length(FE_RV)))

FE_RV_Summary

##################################
# Exploring predictor performance
##################################
dotplot(Predictor ~ RV | Metric, 
        FE_RV_Summary,
        origin = 0,
        type = c("p", "h"),
        pch = 16,
        cex = 2,
        alpha = 0.45,
        prepanel = function(x, y) {
            list(ylim = levels(reorder(y, x)))
        },
        panel = function(x, y, ...) {
            panel.dotplot(x, reorder(y, x), ...)
        })
```

</details>

###  1.3.6 Model Development and Performance Estimation
|
|

####  1.3.6.1 Stochastic Gradient Boosting (GBM)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.1, warning=FALSE, message=FALSE}
##################################
# Preparing the dataset for
# model development and test
##################################
set.seed(12345678)
trainIndex <- createDataPartition(PME$LIFEXP,
                                  p = 0.8, 
                                  list = FALSE, 
                                  times = 1)

##################################
# Formulating the model development data
##################################
MD <- PME[ trainIndex,]

##################################
# Formulating the model test data
##################################
MT <- PME[-trainIndex,]

##################################
# Preparing the dataset for
# model development
##################################
MD <- MD[,c("GENDER","CONTIN","INFMOR","PERCAP","CLTECH","NCOMOR","LIFEXP")]

MD.Model.Predictors <- MD[,c("GENDER","CONTIN","INFMOR","PERCAP","CLTECH","NCOMOR")]

##################################
# Preparing the dataset for
# model test
##################################
MT <- MT[,c("GENDER","CONTIN","INFMOR","PERCAP","CLTECH","NCOMOR","LIFEXP")]

MT.Model.Predictors <- MT[,c("GENDER","CONTIN","INFMOR","PERCAP","CLTECH","NCOMOR")]

##################################
# Creating consistent fold assignments
# for the 10-Fold Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(MD$LIFEXP,
                             k = 10,
                             returnTrain=TRUE)
KFold_Control <- trainControl(method="cv",
                              index=KFold_Indices)

##################################
# Defining the model hyperparameter values
# for the GBM model
##################################
GBM_Grid = expand.grid(n.trees = c(100, 200, 300),
                       interaction.depth = c(1, 3, 5),
                       shrinkage = c(0.10,0.05,0.01),
                       n.minobsinnode = c(15,10,5))

##################################
# Running the GBM model
# by setting the caret method to 'gbm'
##################################
set.seed(12345678)
GBM_Tune <- train(x = MD.Model.Predictors,
                  y = MD$LIFEXP,
                  method = "gbm",
                  tuneGrid = GBM_Grid,
                  trControl = KFold_Control)

##################################
# Reporting the apparent results
# for the GBM model
##################################
GBM_DALEX <- DALEX::explain(GBM_Tune,
                            data = MD.Model.Predictors,
                            y = MD$LIFEXP,
                            verbose = FALSE,
                            label = "GBM")

(GBM_DALEX_Performance <- model_performance(GBM_DALEX))
(GBM_DALEX_Diagnostics <- model_diagnostics(GBM_DALEX))
plot(GBM_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("GBM: Observed and Predicted LIFEXP")

GBM_DALEX_VariableImportance    <- model_parts(GBM_DALEX,
                                               loss_function = loss_root_mean_square,
                                               B = 200,
                                               N = NULL)

plot(GBM_DALEX_VariableImportance)

##################################
# Reporting the cross-validation results
# for the GBM model
##################################
GBM_Tune

GBM_Tune$finalModel

(GBM_Tune_RMSE <- GBM_Tune$results[GBM_Tune$results$shrinkage==GBM_Tune$bestTune$shrinkage &
                              GBM_Tune$results$interaction.depth==GBM_Tune$bestTune$interaction.depth &
                              GBM_Tune$results$n.minobsinnode==GBM_Tune$bestTune$n.minobsinnode &
                              GBM_Tune$results$n.trees==GBM_Tune$bestTune$n.trees,
                 c("RMSE")])

(GBM_Tune_Rsquared <- GBM_Tune$results[GBM_Tune$results$shrinkage==GBM_Tune$bestTune$shrinkage &
                              GBM_Tune$results$interaction.depth==GBM_Tune$bestTune$interaction.depth &
                              GBM_Tune$results$n.minobsinnode==GBM_Tune$bestTune$n.minobsinnode &
                              GBM_Tune$results$n.trees==GBM_Tune$bestTune$n.trees,
                 c("Rsquared")])

(GBM_Tune_MAE <- GBM_Tune$results[GBM_Tune$results$shrinkage==GBM_Tune$bestTune$shrinkage &
                              GBM_Tune$results$interaction.depth==GBM_Tune$bestTune$interaction.depth &
                              GBM_Tune$results$n.minobsinnode==GBM_Tune$bestTune$n.minobsinnode &
                              GBM_Tune$results$n.trees==GBM_Tune$bestTune$n.trees,
                 c("MAE")])

```

</details>

|
|
####  1.3.6.2 Random Forest (RF)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.2, warning=FALSE, message=FALSE}
##################################
# Defining the model hyperparameter values
# for the RF model
##################################
RF_Grid = data.frame(mtry = c(100, 200, 300, 400, 500,
                              600, 700, 800, 900, 1000))

##################################
# Running the RF model
# by setting the caret method to 'RF'
##################################
set.seed(12345678)
RF_Tune <- train(x = MD.Model.Predictors,
                 y = MD$LIFEXP,
                 method = "rf",
                 tuneGrid = RF_Grid,
                 trControl = KFold_Control)

##################################
# Reporting the apparent results
# for the RF model
##################################
RF_DALEX <- DALEX::explain(RF_Tune,
                           data = MD.Model.Predictors,
                           y = MD$LIFEXP,
                           verbose = FALSE,
                           label = "RF")

(RF_DALEX_Performance <- model_performance(RF_DALEX))
(RF_DALEX_Diagnostics <- model_diagnostics(RF_DALEX))
plot(RF_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("RF: Observed and Predicted LIFEXP")

RF_DALEX_VariableImportance    <- model_parts(RF_DALEX,
                                              loss_function = loss_root_mean_square,
                                              B = 200,
                                              N = NULL)

plot(RF_DALEX_VariableImportance)

##################################
# Reporting the cross-validation results
# for the RF model
##################################
RF_Tune

RF_Tune$finalModel

(RF_Tune_RMSE <- RF_Tune$results[RF_Tune$results$mtry==RF_Tune$bestTune$mtry,
                 c("RMSE")])

(RF_Tune_Rsquared <- RF_Tune$results[RF_Tune$results$mtry==RF_Tune$bestTune$mtry,
                 c("Rsquared")])

(RF_Tune_MAE <- RF_Tune$results[RF_Tune$results$mtry==RF_Tune$bestTune$mtry,
                 c("MAE")])

```

</details>

|
|
####  1.3.6.3 Neural Network (NN)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.3, warning=FALSE, message=FALSE}
##################################
# Defining the model hyperparameter values
# for the NN model
##################################
NN_Grid = expand.grid(size = c(2, 5, 10, 15, 20),
                      decay = c(0, 0.1, 0.001, 0.0001, 0.00001))

##################################
# Running the NN model
# by setting the caret method to 'NN'
##################################
set.seed(12345678)
NN_Tune <- train(x = MD.Model.Predictors,
                 y = MD$LIFEXP,
                 method = "nnet",
                 linout = TRUE,
                 preProcess = c('center', 'scale'),
                 maxit = 500,
                 tuneGrid = NN_Grid,
                 trControl = KFold_Control)

##################################
# Reporting the apparent results
# for the NN model
##################################
NN_DALEX <- DALEX::explain(NN_Tune,
                           data = MD.Model.Predictors,
                           y = MD$LIFEXP,
                           verbose = FALSE,
                           label = "NN")

(NN_DALEX_Performance <- model_performance(NN_DALEX))
(NN_DALEX_Diagnostics <- model_diagnostics(NN_DALEX))
plot(NN_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("RF: Observed and Predicted LIFEXP")

NN_DALEX_VariableImportance    <- model_parts(NN_DALEX,
                                              loss_function = loss_root_mean_square,
                                              B = 200,
                                              N = NULL)

plot(NN_DALEX_VariableImportance)

##################################
# Reporting the cross-validation results
# for the NN model
##################################
NN_Tune

NN_Tune$finalModel

(NN_Tune_RMSE <- NN_Tune$results[NN_Tune$results$size==NN_Tune$bestTune$size &
                              NN_Tune$results$decay==NN_Tune$bestTune$decay,
                 c("RMSE")])

(NN_Tune_Rsquared <- NN_Tune$results[NN_Tune$results$size==NN_Tune$bestTune$size &
                              NN_Tune$results$decay==NN_Tune$bestTune$decay,
                 c("Rsquared")])

(NN_Tune_MAE <- NN_Tune$results[NN_Tune$results$size==NN_Tune$bestTune$size &
                              NN_Tune$results$decay==NN_Tune$bestTune$decay,
                 c("MAE")])

```

</details>

|
|
####  1.3.6.4 Partial Least Squares Regression (PLS)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.4, warning=FALSE, message=FALSE}
##################################
# Defining the model hyperparameter values
# for the PLS model
##################################
PLS_Grid = expand.grid(ncomp = 1:5)

##################################
# Running the PLS model
# by setting the caret method to 'pls'
##################################
set.seed(12345678)
PLS_Tune <- train(x = MD.Model.Predictors,
                  y = MD$LIFEXP,
                  method = "pls",
                  tuneGrid = PLS_Grid,
                  trControl = KFold_Control)

##################################
# Reporting the apparent results
# for the PLS model
##################################
PLS_DALEX <- DALEX::explain(PLS_Tune,
                           data = MD.Model.Predictors,
                           y = MD$LIFEXP,
                           verbose = FALSE,
                           label = "PLS")

(PLS_DALEX_Performance <- model_performance(PLS_DALEX))
(PLS_DALEX_Diagnostics <- model_diagnostics(PLS_DALEX))
plot(PLS_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("RF: Observed and Predicted LIFEXP")

PLS_DALEX_VariableImportance    <- model_parts(PLS_DALEX,
                                              loss_function = loss_root_mean_square,
                                              B = 200,
                                              N = NULL)

plot(PLS_DALEX_VariableImportance)

##################################
# Reporting the cross-validation results
# for the PLS model
##################################
PLS_Tune

PLS_Tune$finalModel

(PLS_Tune_RMSE <- PLS_Tune$results[PLS_Tune$results$ncomp==PLS_Tune$bestTune$ncomp,
                 c("RMSE")])

(PLS_Tune_Rsquared <- PLS_Tune$results[PLS_Tune$results$ncomp==PLS_Tune$bestTune$ncomp,
                 c("Rsquared")])

(PLS_Tune_MAE <- PLS_Tune$results[PLS_Tune$results$ncomp==PLS_Tune$bestTune$ncomp,
                 c("MAE")])

```

</details>

|
|
####  1.3.6.5 Cubist Regression (CUBIST)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.5, warning=FALSE, message=FALSE}
##################################
# Defining the model hyperparameter values
# for the CUBIST model
##################################
CUBIST_Grid = expand.grid(committees = c(10, 20, 30, 40, 50),
                          neighbors = c(0, 3, 6, 9))


##################################
# Running the CUBIST model
# by setting the caret method to 'cubist'
##################################
set.seed(12345678)
CUBIST_Tune <- train(x = MD.Model.Predictors,
                   y = MD$LIFEXP,
                   method = "cubist",
                   tuneGrid = CUBIST_Grid,
                   trControl = KFold_Control)

##################################
# Reporting the apparent results
# for the CUBIST model
##################################
CUBIST_DALEX <- DALEX::explain(CUBIST_Tune,
                           data = MD.Model.Predictors,
                           y = MD$LIFEXP,
                           verbose = FALSE,
                           label = "CUBIST")

(CUBIST_DALEX_Performance <- model_performance(CUBIST_DALEX))
(CUBIST_DALEX_Diagnostics <- model_diagnostics(CUBIST_DALEX))
plot(CUBIST_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("RF: Observed and Predicted LIFEXP")

CUBIST_DALEX_VariableImportance    <- model_parts(CUBIST_DALEX,
                                              loss_function = loss_root_mean_square,
                                              B = 200,
                                              N = NULL)

plot(CUBIST_DALEX_VariableImportance)

##################################
# Reporting the cross-validation results
# for the CUBIST model
##################################
CUBIST_Tune

CUBIST_Tune$finalModel

(CUBIST_Tune_RMSE <- CUBIST_Tune$results[CUBIST_Tune$results$committees==CUBIST_Tune$bestTune$committees &
                              CUBIST_Tune$results$neighbors==CUBIST_Tune$bestTune$neighbors,
                 c("RMSE")])

(CUBIST_Tune_Rsquared <- CUBIST_Tune$results[CUBIST_Tune$results$committees==CUBIST_Tune$bestTune$committees &
                              CUBIST_Tune$results$neighbors==CUBIST_Tune$bestTune$neighbors,
                 c("Rsquared")])

(CUBIST_Tune_MAE <- CUBIST_Tune$results[CUBIST_Tune$results$committees==CUBIST_Tune$bestTune$committees &
                              CUBIST_Tune$results$neighbors==CUBIST_Tune$bestTune$neighbors,
                 c("MAE")])

```

</details>

###  1.3.7 Model Performance Validation
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.7, warning=FALSE, message=FALSE}
##################################
# Evaluating the models
# on the model test data
##################################

##################################
# Formulating the DALEX object
# for the Best GBM model
# as applied to the model test data
##################################
GBM_DALEX <- DALEX::explain(GBM_Tune,
                            data = MT.Model.Predictors,
                            y = MT$LIFEXP,
                            verbose = FALSE,
                            label = "GBM")

(GBM_DALEX_Performance <- model_performance(GBM_DALEX))
(GBM_DALEX_Diagnostics <- model_diagnostics(GBM_DALEX))
plot(GBM_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("GBM: Observed and Predicted LIFEXP")

##################################
# Formulating the DALEX object
# for the Best RF model
# as applied to the model test data
##################################
RF_DALEX <- DALEX::explain(RF_Tune,
                           data = MT.Model.Predictors,
                           y = MT$LIFEXP,
                           verbose = FALSE,
                           label = "RF")

(RF_DALEX_Performance <- model_performance(RF_DALEX))
(RF_DALEX_Diagnostics <- model_diagnostics(RF_DALEX))
plot(RF_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("RF: Observed and Predicted LIFEXP")

##################################
# Formulating the DALEX object
# for the Best NN model
# as applied to the model test data
##################################
NN_DALEX <- DALEX::explain(NN_Tune,
                           data = MT.Model.Predictors,
                           y = MT$LIFEXP,
                           verbose = FALSE,
                           label = "NN")

(NN_DALEX_Performance <- model_performance(NN_DALEX))
(NN_DALEX_Diagnostics <- model_diagnostics(NN_DALEX))
plot(NN_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("NN: Observed and Predicted LIFEXP")

##################################
# Formulating the DALEX object
# for the Best PLS model
# as applied to the model test data
##################################
PLS_DALEX <- DALEX::explain(PLS_Tune,
                            data = MT.Model.Predictors,
                            y = MT$LIFEXP,
                            verbose = FALSE,
                            label = "PLS")

(PLS_DALEX_Performance <- model_performance(PLS_DALEX))
(PLS_DALEX_Diagnostics <- model_diagnostics(PLS_DALEX))
plot(PLS_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("PLS: Observed and Predicted LIFEXP")

##################################
# Formulating the DALEX object
# for the Best CUBIST model
# as applied to the model test data
##################################
CUBIST_DALEX <- DALEX::explain(CUBIST_Tune,
                            data = MT.Model.Predictors,
                            y = MT$LIFEXP,
                            verbose = FALSE,
                            label = "CUBIST")

(CUBIST_DALEX_Performance <- model_performance(CUBIST_DALEX))
(CUBIST_DALEX_Diagnostics <- model_diagnostics(CUBIST_DALEX))
plot(CUBIST_DALEX_Diagnostics,
     variable = "y",
     yvariable = "y_hat") +
  geom_point(size=3) +
  scale_x_continuous("Observed LIFEXP") +
  scale_y_continuous("Predicted LIFEXP") +
  geom_abline(slope = 1) +
  ggtitle("CUBIST: Observed and Predicted LIFEXP")

```

</details>

###  1.3.8 Model Selection
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.8, warning=FALSE, message=FALSE}
##################################
# Consolidating the performance
# on the model test data
##################################
plot(GBM_DALEX_Performance,
     RF_DALEX_Performance,
     NN_DALEX_Performance,
     PLS_DALEX_Performance,
     CUBIST_DALEX_Performance)


plot(GBM_DALEX_Performance,
     RF_DALEX_Performance,
     NN_DALEX_Performance,
     PLS_DALEX_Performance,
     CUBIST_DALEX_Performance,
     geom = "boxplot")

plot(GBM_DALEX_Performance,
     RF_DALEX_Performance,
     NN_DALEX_Performance,
     PLS_DALEX_Performance,
     CUBIST_DALEX_Performance,
     geom = "histogram")

##################################
# Consolidating the variable importance
# on the model test data
##################################
GBM_DALEX_VariableImportance    <- model_parts(GBM_DALEX,
                                               loss_function = loss_root_mean_square,
                                               B = 200,
                                               N = NULL)
RF_DALEX_VariableImportance     <- model_parts(RF_DALEX,
                                               loss_function = loss_root_mean_square,
                                               B = 200,
                                               N = NULL)
NN_DALEX_VariableImportance     <- model_parts(NN_DALEX,
                                               loss_function = loss_root_mean_square,
                                               B = 200,
                                               N = NULL)
PLS_DALEX_VariableImportance    <- model_parts(PLS_DALEX,
                                               loss_function = loss_root_mean_square,
                                               B = 200,
                                               N = NULL)
CUBIST_DALEX_VariableImportance <- model_parts(CUBIST_DALEX,
                                               loss_function = loss_root_mean_square,
                                               B = 200,
                                               N = NULL)

plot(GBM_DALEX_VariableImportance,
     RF_DALEX_VariableImportance,
     NN_DALEX_VariableImportance,
     PLS_DALEX_VariableImportance,
     CUBIST_DALEX_VariableImportance)

```

</details>

###  1.3.9 Model Post-Hoc Analysis
|
|
####  1.3.9.1 Dataset Level Exploration : Variable Importance (DLE_VARIMP)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.1, warning=FALSE, message=FALSE}
##################################
# Summarizing the variable importance
# for the final model - GBM
##################################
GBM_DALEX_VariableImportance

plot(GBM_DALEX_VariableImportance)

```

</details>

|
|
####  1.3.9.2 Dataset Level Exploration : Partial Dependence Plots (DLE_PDP)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.2, warning=FALSE, message=FALSE}
##################################
# Formulating the partial dependence plots
# for the final model - GBM
# using the numeric variables
##################################
GBM_DALEX_PartialDependencePlot_INFMOR <- model_profile(GBM_DALEX,
                                                        variables = "INFMOR")
GBM_DALEX_PartialDependencePlot_NCOMOR <- model_profile(GBM_DALEX,
                                                        variables = "NCOMOR")
GBM_DALEX_PartialDependencePlot_CLTECH <- model_profile(GBM_DALEX,
                                                        variables = "CLTECH")
GBM_DALEX_PartialDependencePlot_PERCAP <- model_profile(GBM_DALEX,
                                                        variables = "PERCAP")

(GBM_DALEX_PDP_INFMOR <- plot(GBM_DALEX_PartialDependencePlot_INFMOR,
                              geom = "profiles"))
(GBM_DALEX_PDP_NCOMOR <- plot(GBM_DALEX_PartialDependencePlot_NCOMOR,
                              geom = "profiles"))
(GBM_DALEX_PDP_CLTECH <- plot(GBM_DALEX_PartialDependencePlot_CLTECH,
                              geom = "profiles"))
(GBM_DALEX_PDP_PERCAP <- plot(GBM_DALEX_PartialDependencePlot_PERCAP,
                              geom = "profiles"))

##################################
# Formulating the grouped partial dependence plots
# for the final model - GBM
# using the numeric variables
# stratified by GENDER
##################################
GBM_DALEX_GroupedPartialDependencePlot_INFMOR <- model_profile(GBM_DALEX,
                                                               variables = "INFMOR",
                                                               groups = "GENDER")
GBM_DALEX_GroupedPartialDependencePlot_NCOMOR <- model_profile(GBM_DALEX,
                                                               variables = "NCOMOR",
                                                               groups = "GENDER")
GBM_DALEX_GroupedPartialDependencePlot_CLTECH <- model_profile(GBM_DALEX,
                                                               variables = "CLTECH",
                                                               groups = "GENDER")
GBM_DALEX_GroupedPartialDependencePlot_PERCAP <- model_profile(GBM_DALEX,
                                                               variables = "PERCAP",
                                                               groups = "GENDER")

(GBM_DALEX_GPDP_INFMOR <- plot(GBM_DALEX_GroupedPartialDependencePlot_INFMOR,
                              geom = "profiles"))
(GBM_DALEX_GPDP_NCOMOR <- plot(GBM_DALEX_GroupedPartialDependencePlot_NCOMOR,
                              geom = "profiles"))
(GBM_DALEX_GPDP_CLTECH <- plot(GBM_DALEX_GroupedPartialDependencePlot_CLTECH,
                              geom = "profiles"))
(GBM_DALEX_GPDP_PERCAP <- plot(GBM_DALEX_GroupedPartialDependencePlot_PERCAP,
                              geom = "profiles"))

##################################
# Formulating the grouped partial dependence plots
# for the final model - GBM
# using the numeric variables
# stratified by CONTIN
##################################
GBM_DALEX_GroupedPartialDependencePlot_INFMOR <- model_profile(GBM_DALEX,
                                                               variables = "INFMOR",
                                                               groups = "CONTIN")
GBM_DALEX_GroupedPartialDependencePlot_NCOMOR <- model_profile(GBM_DALEX,
                                                               variables = "NCOMOR",
                                                               groups = "CONTIN")
GBM_DALEX_GroupedPartialDependencePlot_CLTECH <- model_profile(GBM_DALEX,
                                                               variables = "CLTECH",
                                                               groups = "CONTIN")
GBM_DALEX_GroupedPartialDependencePlot_PERCAP <- model_profile(GBM_DALEX,
                                                               variables = "PERCAP",
                                                               groups = "CONTIN")

(GBM_DALEX_GPDP_INFMOR <- plot(GBM_DALEX_GroupedPartialDependencePlot_INFMOR,
                              geom = "profiles"))
(GBM_DALEX_GPDP_NCOMOR <- plot(GBM_DALEX_GroupedPartialDependencePlot_NCOMOR,
                              geom = "profiles"))
(GBM_DALEX_GPDP_CLTECH <- plot(GBM_DALEX_GroupedPartialDependencePlot_CLTECH,
                              geom = "profiles"))
(GBM_DALEX_GPDP_PERCAP <- plot(GBM_DALEX_GroupedPartialDependencePlot_PERCAP,
                              geom = "profiles"))

##################################
# Formulating the partial dependence plots
# for the final model - GBM
# using the factor variables
##################################
GBM_DALEX_PartialDependencePlot_GENDER <- model_profile(GBM_DALEX,
                                                        variable_type = 'categorical',
                                                        variables = "GENDER")
GBM_DALEX_PartialDependencePlot_CONTIN <- model_profile(GBM_DALEX,
                                                        variable_type = 'categorical',
                                                        variables = "CONTIN")

(GBM_DALEX_PDP_GENDER <- plot(GBM_DALEX_PartialDependencePlot_GENDER,
                               geom = "profiles"))
(GBM_DALEX_PDP_CONTIN <- plot(GBM_DALEX_PartialDependencePlot_CONTIN,
                               geom = "profiles"))

```

</details>
|
|
####  1.3.9.3 Instance Level Exploration : Breakdown Plots (ILE_BP)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.3, warning=FALSE, message=FALSE}
##################################
# Formulating the sampled instances
# for illustration
##################################
(Instance_1_Philippines_Female  <- PME[PME$COUNTRY=="Philippines" & PME$GENDER=="Female",
                                   c("GENDER","CONTIN","INFMOR","PERCAP","CLTECH","NCOMOR","LIFEXP")])
(Instance_2_Philippines_Male    <- PME[PME$COUNTRY=="Philippines" & PME$GENDER=="Male",
                                   c("GENDER","CONTIN","INFMOR","PERCAP","CLTECH","NCOMOR","LIFEXP")])

##################################
# Obtaining the breakdown plots
# for the individual instances
##################################
(Instance_1_GBM_BDP <- DALEX::predict_parts(explainer = GBM_DALEX,
                                           new_observation = Instance_1_Philippines_Female[,c(1:6)],
                                           type = "break_down"))
plot(Instance_1_GBM_BDP)

(Instance_2_GBM_BDP <- DALEX::predict_parts(explainer = GBM_DALEX,
                                           new_observation = Instance_2_Philippines_Male[,c(1:6)],
                                           type = "break_down"))
plot(Instance_2_GBM_BDP)

```

</details>
|
|
####  1.3.9.4 Instance Level Exploration : Shapley Additive Explanations (ILE_SHAP)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.4, warning=FALSE, message=FALSE}
##################################
# Obtaining the shapley additive explanations
# for the individual instances
##################################
(Instance_1_GBM_SHAP <- DALEX::predict_parts(explainer = GBM_DALEX,
                                           new_observation = Instance_1_Philippines_Female[,c(1:6)],
                                           type = "shap",
                                           B = 25))
plot(Instance_1_GBM_SHAP)

(Instance_2_GBM_SHAP <- DALEX::predict_parts(explainer = GBM_DALEX,
                                           new_observation = Instance_2_Philippines_Male[,c(1:6)],
                                           type = "shap",
                                           B = 25))
plot(Instance_2_GBM_SHAP)

```

</details>

|
|
####  1.3.9.5 Instance Level Exploration : Ceteris Paribus Profiles (ILE_CPP)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.5, warning=FALSE, message=FALSE}
##################################
# Obtaining the ceteris paribus profiles
# for the individual instances
##################################
(Instance_1_GBM_CPP <- DALEX::predict_profile(explainer = GBM_DALEX,
                                           new_observation = Instance_1_Philippines_Female[,c(1:6)]))
plot(Instance_1_GBM_CPP,
     variables = c("INFMOR","PERCAP","CLTECH","NCOMOR")) +
  ggtitle("Ceteris-paribus profile", "") + 
  ylim(55, 80)

plot(Instance_1_GBM_CPP,
     variables = c("GENDER","CONTIN"), 
     variable_type = "categorical", 
     categorical_type = "bars") +
  ggtitle("Ceteris-paribus profile", "")

(Instance_2_GBM_CPP <- DALEX::predict_profile(explainer = GBM_DALEX,
                                           new_observation = Instance_2_Philippines_Male[,c(1:6)]))
plot(Instance_2_GBM_CPP,
     variables = c("INFMOR","PERCAP","CLTECH","NCOMOR")) +
  ggtitle("Ceteris-paribus profile", "") + 
  ylim(55, 80)

plot(Instance_2_GBM_CPP,
     variables = c("GENDER","CONTIN"), 
     variable_type = "categorical", 
     categorical_type = "bars") +
  ggtitle("Ceteris-paribus profile", "")

```

</details>
|
|
####  1.3.9.6 Instance Level Exploration : Local Fidelity Plot (ILE_LFP)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.6, warning=FALSE, message=FALSE}
Instance_1_GBM_LFP <- predict_diagnostics(explainer = GBM_DALEX,
                                         new_observation = Instance_1_Philippines_Female[,c(1:6)],
                                         neighbours = 50)
plot(Instance_1_GBM_LFP)

Instance_2_GBM_LFP <- predict_diagnostics(explainer = GBM_DALEX,
                                         new_observation = Instance_2_Philippines_Male[,c(1:6)],
                                         neighbours = 50)
plot(Instance_2_GBM_LFP)

```

</details>
|
|
####  1.3.9.7 Instance Level Exploration : Local Stability Plot (ILE_LSP)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.7, warning=FALSE, message=FALSE}
Instance_1_GBM_LFP <- predict_diagnostics(explainer = GBM_DALEX,
                                          new_observation = Instance_1_Philippines_Female[,c(1:6)],
                                          neighbours = 5,
                                          variables = c("INFMOR","NCOMOR","CLTECH","PERCAP"))
plot(Instance_1_GBM_LFP)

Instance_1_GBM_LFP <- predict_diagnostics(explainer = GBM_DALEX,
                                          new_observation = Instance_1_Philippines_Female[,c(1:6)],
                                          neighbours = 5,
                                          variables = c("GENDER","CONTIN"))
plot(Instance_1_GBM_LFP)

Instance_2_GBM_LFP <- predict_diagnostics(explainer = GBM_DALEX,
                                          new_observation = Instance_2_Philippines_Male[,c(1:6)],
                                          neighbours = 5,
                                          variables = c("INFMOR","NCOMOR","CLTECH","PERCAP"))
plot(Instance_2_GBM_LFP)

Instance_2_GBM_LFP <- predict_diagnostics(explainer = GBM_DALEX,
                                          new_observation = Instance_2_Philippines_Male[,c(1:6)],
                                          neighbours = 5,
                                          variables = c("GENDER","CONTIN"))
plot(Instance_2_GBM_LFP)

```

</details>

##  1.4 Summary
|
# **2. References**
|
| **[Book]** [Explanatory Model Analysis: Explore, Explain, and Examine Predictive Models With examples in R and Python](https://ema.drwhy.ai/) by Przemyslaw Biecek and Tomasz Burzykowski
| **[Book]** [Explainable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar
| **[Book]** [Explainable AI: Interpreting, Explaining and Visualizing Deep Learning](https://link.springer.com/book/10.1007/978-3-030-28954-6) by Wojciech Samek, Gregoire Montavon, Andrea Vedaldi, Lars Kai Hansen and Klaus-Robert Muller
| **[R Package]** [DALEX](https://cran.r-project.org/web/packages/DALEX/index.html) by Przemyslaw Biecek, Szymon Maksymiuk and Hubert Baniecki
| **[R Package]** [iml](https://cran.r-project.org/web/packages/iml/index.html) by Christoph Molnar
| **[R Package]** [ALEPlot](https://cran.r-project.org/web/packages/ALEPlot/index.html) by Dan Apley
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html) by Leo Breiman, Adele Cutler, Andy Liaw and Matthew Wiener
| **[R Package]** [auditor](https://cran.r-project.org/web/packages/auditor/index.html) by Alicja Gosiewska, Przemyslaw Biecek, Hubert Baniecki and Tomasz MikoÅ‚ajczyk
| **[R Package]** [fastshap](https://cran.r-project.org/web/packages/fastshap/index.html) by Brandon Greenwell
| **[R Package]** [rms](https://cran.r-project.org/web/packages/rms/index.html) by Frank Harrell
| **[R Package]** [EIX](https://cran.r-project.org/web/packages/EIX/index.html) by Szymon Maksymiuk, Ewelina Karbowiak and Przemyslaw Biecek
| **[R Package]** [parsnip](https://cran.r-project.org/web/packages/parsnip/index.html) by Max Kuhn and Davis Vaughan 
| **[R Package]** [h2o](https://cran.r-project.org/web/packages/h2o/index.html) by Tomas Fryda, Erin LeDell, Navdeep Gill, Spencer Aiello, Anqi Fu, Arno Candel, Cliff Click, Tom Kraljevic, Tomas Nykodym, Patrick Aboyoun, Michal Kurka, Michal Malohlava, Sebastien Poirier and Wendy Wong
| **[R Package]** [tidymodels](https://cran.r-project.org/web/packages/tidymodels/index.html) by Max Kuhn and Hadley Wickham 
| **[R Package]** [e1071](https://cran.r-project.org/web/packages/e1071/index.html) by David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel and Friedrich Leisch
| **[R Package]** [lime](https://cran.r-project.org/web/packages/lime/index.html) by Emil Hvitfeldt, Thomas Lin Pedersen and Michael Benesty
| **[R Package]** [ExplainPrediction](https://cran.r-project.org/web/packages/ExplainPrediction/index.html) by Marko Robnik-Sikonja
| **[R Package]** [localModel](https://cran.r-project.org/web/packages/localModel/index.html) by Przemyslaw Biecek and Mateusz Staniak
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [minerva](https://cran.r-project.org/web/packages/minerva/minerva.pdf) by Michele Filosi
| **[R Package]** [CORElearn](https://cran.r-project.org/web/packages/CORElearn/CORElearn.pdf) by Marko Robnik-Sikonja and Petr Savicky
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [gbm](https://cran.r-project.org/web/packages/gbm/index.html) by Brandon Greenwell, Bradley Boehmke, Jay Cunningham and GBM Developers
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) by Andy Liaw
| **[R Package]** [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) by Brian Ripley
| **[R Package]** [pls](https://cran.r-project.org/web/packages/pls/pls.pdf) by Kristian Hovde Liland
| **[R Package]** [Cubist](https://cran.r-project.org/web/packages/Cubist/Cubist.pdf) by Max Kuhn
| **[R Package]** [patchwork](https://cran.r-project.org/web/packages/patchwork/patchwork.pdf) by Thomas Lin Pedersen
| **[Article]** [Interpretation Methods for Black-Box Machine Learning Models in Insurance Rating-Type Applications ](https://support.sas.com/resources/papers/proceedings20/5116-2020.pdf) by Gabe Taylor, Sunish Menon, Huimin Ru, Ray Wright, Xin Hunt and Ralph Abbey
| **[Article]** [4 Model-Agnostic Interpretability Techniques for Complex Models ](https://blogs.sas.com/content/subconsciousmusings/2020/05/07/model-agnostic-interpretability/) by Funda Gunes
| **[Article]** [How Can We Provide Post-Hoc Explanations for Black-Box AI Models? ](https://aisingapore.github.io/ai-practitioner-handbook/book/6-modelling/post-hoc-explanation.html) by Joy Lin
| **[Publication]** [Stochastic Gradient Boosting](https://www.sciencedirect.com/science/article/abs/pii/S0167947301000652) by Jerome Friedman (Computational Statistics and Data Analysis)
| **[Publication]** [Random Forest](https://link.springer.com/article/10.1023/A:1010933404324) by Leo Breiman (Machine Learning)
| **[Publication]** [The Collinearity Problem in Linear Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses](https://epubs.siam.org/doi/10.1137/0905052) by Svante Wold, Axel Ruhe, Herman Wold, and William Dunn (Society for Industrial and Applied Mathematics)
| **[Publication]** [Learning With Continuous Classes ](https://www.semanticscholar.org/paper/Learning-With-Continuous-Classes-Quinlan/ead572634c6f7253bf187a3e9a7dc87ae2e34258) by Ross Quinlan (Proceedings of the 5th Australian Joint Conference On Artificial Intelligence)
| **[Publication]** [A Survey of Methods for Explaining Black Box Models](https://dl.acm.org/doi/10.1145/3236009) by Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti and Dino Pedreschi (ACM Computing Surveys)
| **[Publication]** [iml: An R package for Interpretable Machine Learning](https://doi.org/10.21105/joss.00786) by Christoph Molnar (Journal of Open Source Software)
|
|
|
|
